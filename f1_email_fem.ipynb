{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## from raw email data prepare groupby email fre feature data.\n",
    "# prepare user, community, and peer data dic\n",
    "# for each calculate anomaly and plot\n",
    "\n",
    "#imports\n",
    "import pandas as pd\n",
    "from helper import pickle_store, pickle_restore\n",
    "import dateutil\n",
    "from adem import *\n",
    "from collections import Counter \n",
    "\n",
    "def fetch_raw_data():\n",
    "    #fetch all rawdata and save as pickle format for speed.\n",
    "    email_raw_df = pd.read_csv(\"r4.2/email.csv\") # 2629979 rows, 11 columns\n",
    "\n",
    "    # Select small data till 1 April\n",
    "    # email_raw_df = email_raw_df[:515658]\n",
    "    pickle_store(\"pickle/email_rawdf_file\", email_raw_df) #raw means not indexed by Timeindex\n",
    "\n",
    "# fetch_raw_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "done heavy part...\n"
     ]
    }
   ],
   "source": [
    "# feature1: email frequency. function will ingest raw data and return a dict ready to be fed to anomaly detection module\n",
    "# returned diction will be of form: d= {user:df}. user activity would only be taken for weekdays.\n",
    "# special cases: What if employee is on leave on some days.. then activity will 0.\n",
    "# steps: 1. take raw data, reduce the data columns to activity of interest 2. groupby user-- each user will have its own df now.\n",
    "# 3. groupby on activity to cal frequency\n",
    "\n",
    "#get email data of first 3 months: data is without date column converted to timedates type\n",
    "email_raw_df = pickle_restore(\"pickle/email_rawdf_file\")\n",
    "\n",
    "email_freq_feature_df=email_raw_df[['user', 'date', 'from']]\n",
    "\n",
    "email_freq_feature_df['date'] = pd.to_datetime(email_freq_feature_df['date'])\n",
    "mask = (email_freq_feature_df['date'] <= end_d) & (email_freq_feature_df['date'] >= start_d)\n",
    "email_freq_feature_df = email_freq_feature_df.loc[mask]\n",
    "\n",
    "print(\"done heavy part...\")\n",
    "\n",
    "#create a copy of orig df\n",
    "email_freq_feature_cmnty_df = email_freq_feature_df.copy()\n",
    "email_freq_feature_peer_df = email_freq_feature_df.copy()\n",
    "\n",
    "# 1. in email_freq_feature_df add community and peer column with id as values.\n",
    "usr_cmnty_map = pickle_restore(\"pickle/community_louvian_file\")\n",
    "email_freq_feature_cmnty_df['cmnty']=email_freq_feature_cmnty_df.apply(lambda row: usr_cmnty_map[row.user], axis=1)\n",
    "\n",
    "usr_peer_map = pickle_restore(\"pickle/eid_role_map_file\")\n",
    "email_freq_feature_peer_df['peer']=email_freq_feature_peer_df.apply(lambda row: usr_peer_map[row.user], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date back to string format\n",
    "email_freq_feature_df['date']=email_freq_feature_df['date'].astype(str)\n",
    "email_freq_feature_cmnty_df['date']=email_freq_feature_cmnty_df['date'].astype(str)\n",
    "email_freq_feature_peer_df['date']=email_freq_feature_peer_df['date'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prep_cbp_dic(email_freq_feature_cmnty_df):\n",
    "    #convert date column of srt type to Timestamp type.\n",
    "    email_freq_feature_cmnty_df['date'] = email_freq_feature_cmnty_df['date'].apply(dateutil.parser.parse, dayfirst=True)\n",
    "    #now convert date column of Timestamptype to only date values and remove hr:mm:ss\n",
    "    email_freq_feature_cmnty_df['date'] = pd.to_datetime(email_freq_feature_cmnty_df['date']).dt.date\n",
    "\n",
    "    # create user, date wise groups and count unique dates then add a separate column for the counts\n",
    "    email_freq_feature_cmnty_df=email_freq_feature_cmnty_df.groupby(['cmnty', 'date']).date.agg('count').to_frame('email_freq').reset_index()\n",
    "\n",
    "    cmnty_len = Counter(usr_cmnty_map.values())\n",
    "    #make values of email freq avg of community\n",
    "    email_freq_feature_cmnty_df['email_freq'] = email_freq_feature_cmnty_df.apply( lambda x: x.email_freq/cmnty_len[x.cmnty], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    #groupby user now To populate a dictionary for each user.\n",
    "    grp=email_freq_feature_cmnty_df.groupby(['cmnty'])\n",
    "\n",
    "    email_freq_feature_cmnty_dic={} #user:df with feature value\n",
    "    #iterating groups\n",
    "    for name, group in grp:\n",
    "        # print (name)\n",
    "        # print (group)\n",
    "        email_freq_feature_cmnty_dic[name]=group\n",
    "        # ldf = group.groupby(['date'], as_index=False)['cntr'].size()\n",
    "\n",
    "\n",
    "\n",
    "    return email_freq_feature_cmnty_dic\n",
    "\n",
    "def prep_pbp_dic(email_freq_feature_peer_df):\n",
    "    #convert date column of srt type to Timestamp type.\n",
    "    email_freq_feature_peer_df['date'] = email_freq_feature_peer_df['date'].apply(dateutil.parser.parse, dayfirst=True)\n",
    "    #now convert date column of Timestamptype to only date values and remove hr:mm:ss\n",
    "    email_freq_feature_peer_df['date'] = pd.to_datetime(email_freq_feature_peer_df['date']).dt.date\n",
    "\n",
    "    # create user, date wise groups and count unique dates then add a separate column for the counts\n",
    "    email_freq_feature_peer_df=email_freq_feature_peer_df.groupby(['peer', 'date']).date.agg('count').to_frame('email_freq').reset_index()\n",
    "\n",
    "    cmnty_len = Counter(usr_peer_map.values())\n",
    "    #make values of email freq avg of community\n",
    "    email_freq_feature_peer_df['email_freq'] = email_freq_feature_peer_df.apply( lambda x: x.email_freq/cmnty_len[x.peer], axis=1)\n",
    "\n",
    "    #groupby user now To populate a dictionary for each user.\n",
    "    grp=email_freq_feature_peer_df.groupby(['peer'])\n",
    "\n",
    "    email_freq_feature_peer_dic={} #user:df with feature value\n",
    "    #iterating groups\n",
    "    for name, group in grp:\n",
    "        # print (name)\n",
    "        # print (group)\n",
    "        email_freq_feature_peer_dic[name]=group\n",
    "        # ldf = group.groupby(['date'], as_index=False)['cntr'].size()\n",
    "    return email_freq_feature_peer_dic\n",
    "\n",
    "def prep_ubp_dic(email_freq_feature_df):\n",
    "    # shorten the data for debug\n",
    "    # tmp_email_freq_feature_df = email_freq_feature_df #[:1000]\n",
    "\n",
    "    #convert date column of srt type to Timestamp type.\n",
    "    email_freq_feature_df['date'] = email_freq_feature_df['date'].apply(dateutil.parser.parse, dayfirst=True)\n",
    "    #now convert date column of Timestamptype to only date values and remove hr:mm:ss\n",
    "    email_freq_feature_df['date'] = pd.to_datetime(email_freq_feature_df['date']).dt.date\n",
    "\n",
    "    #creates user, date wise groups and each internal group is a dataframe.\n",
    "    # grp=tmp_email_freq_feature_df.groupby(['user', 'date'])\n",
    "\n",
    "    # create user, date wise groups and count unique dates then add a separate column for the counts\n",
    "    email_freq_feature_df=email_freq_feature_df.groupby(['user', 'date']).date.agg('count').to_frame('email_freq').reset_index()\n",
    "\n",
    "    #groupby user now To populate a dictionary for each user.\n",
    "    grp=email_freq_feature_df.groupby(['user'])\n",
    "\n",
    "    email_freq_feature_dic={} #user:df with feature value\n",
    "    #iterating groups\n",
    "    for name, group in grp:\n",
    "        # print (name)\n",
    "        # print (group)\n",
    "        email_freq_feature_dic[name]=group\n",
    "        # ldf = group.groupby(['date'], as_index=False)['cntr'].size()\n",
    "    \n",
    "    return email_freq_feature_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #UBP anom calculation and plot generation:\n",
    "# # 1. Take prepared feature dataframe and \n",
    "# # 2. index by date\n",
    "# # 3. populate missing dates\n",
    "# # 4. get X and Y and call ADEM.\n",
    "\n",
    "# def anom_calc_ubp(df, feature_name, uname, ws=10, sig=3):\n",
    "#     # index email data by date\n",
    "#     df['date'] = pd.to_datetime(df['date'])\n",
    "#     df.set_index('date', inplace=True)\n",
    "\n",
    "#     #Fill the missing dates and corresponding feature values as 0.\n",
    "#     df = df.resample('D').sum().fillna(0)\n",
    "\n",
    "\n",
    "#     X = df.index.to_series() #debug_df['date'] # data_as_frame['Months']\n",
    "#     Y = df[feature_name] #data_as_frame['SunSpots']\n",
    "\n",
    "#     # # plot the results\n",
    "#     # plot_results(X, y=Y, window_size=20, text_xlabel=\"Months\", sigma_value=3, text_ylabel=\"No. of Sun spots\")\n",
    "\n",
    "#     events = explain_anomalies(X, Y, window_size=ws, sigma=sig)\n",
    "#     # Display the anomaly dict\n",
    "#     print(\"Information about the anomalies model:{}\".format(events))\n",
    "#     print()\n",
    "\n",
    "#     events_rolling = explain_anomalies_rolling_std(X, Y, window_size=ws, sigma=sig)\n",
    "#     # Display the anomaly dict\n",
    "#     print(\"Information about the anomalies model rolling:{}\".format(events_rolling))\n",
    "#     print()\n",
    "\n",
    "#     #####################################################################################################\n",
    "\n",
    "#     plot_res (events, Y, baseline_type='UBP', feature_type=feature_name, user_name=uname)\n",
    "#     # plot_res (events_rolling, Y, baseline_type='UBP-rolling', feature_type=feature_name)\n",
    "\n",
    "# # email_freq_feature_dic.keys()\n",
    "# # Feature data format:  user: df [user, date, featurecolmn]\n",
    "# def anom_calc_cbp(df, df_cmnty, feature_name, uname, ws=10, sig=3):\n",
    "#     df_cmnty['date'] = pd.to_datetime(df_cmnty['date'])\n",
    "#     df_cmnty.set_index('date', inplace=True)\n",
    "#     #Fill the missing dates and corresponding feature values as 0.\n",
    "#     df_cmnty = df_cmnty.resample('D').sum().fillna(0)\n",
    "\n",
    "#     baseline_df = df_cmnty\n",
    "    \n",
    "#     # def anom_calc_cbp(df, baseline_df, feature_name, uname):\n",
    "#     # index email data by date\n",
    "#     df['date'] = pd.to_datetime(df['date'])\n",
    "#     df.set_index('date', inplace=True)\n",
    "\n",
    "#     #Fill the missing dates and corresponding feature values as 0.\n",
    "#     df = df.resample('D').sum().fillna(0)\n",
    "\n",
    "#     #assuming baseline_x is date indexed and missed dates already filled\n",
    "#     X = baseline_df[feature_name] #debug_df['date'] # data_as_frame['Months']\n",
    "#     Y = df[feature_name] #data_as_frame['SunSpots']\n",
    "\n",
    "#     # # plot the results\n",
    "#     # plot_results(X, y=Y, window_size=20, text_xlabel=\"Months\", sigma_value=3, text_ylabel=\"No. of Sun spots\")\n",
    "\n",
    "#     events = explain_anomalies_cmnty(X, Y, window_size=ws, sigma=sig)\n",
    "#     # Display the anomaly dict\n",
    "#     print(\"Information about the anomalies model:{}\".format(events))\n",
    "#     print()\n",
    "\n",
    "#     events_rolling = explain_anomalies_rolling_std_cmnty(X, Y, window_size=ws, sigma=sig)\n",
    "#     # Display the anomaly dict\n",
    "#     print(\"Information about the anomalies model rolling:{}\".format(events_rolling))\n",
    "#     print()\n",
    "\n",
    "#     #####################################################################################################\n",
    "\n",
    "#     plot_res_cmnty (events, X, Y, baseline_type='CBP', feature_type=feature_name, user_name=uname)\n",
    "#     # plot_res (events_rolling, Y, baseline_type='UBP-rolling', feature_type=feature_name)\n",
    "\n",
    "# # email_freq_feature_dic.keys()\n",
    "# # Feature data format:  user: df [user, date, featurecolmn]\n",
    "# def anom_calc_pbp(df, df_peer, feature_name, uname, ws=10, sig=3):\n",
    "#     df_peer['date'] = pd.to_datetime(df_peer['date'])\n",
    "#     df_peer.set_index('date', inplace=True)\n",
    "#     #Fill the missing dates and corresponding feature values as 0.\n",
    "#     df_peer = df_peer.resample('D').sum().fillna(0)\n",
    "\n",
    "#     # anom_calc_cbp(df, df_cmnty, 'email_freq', 'AAM0658')\n",
    "\n",
    "#     baseline_df = df_peer\n",
    "\n",
    "#     # def anom_calc_cbp(df, baseline_df, feature_name, uname):\n",
    "#     # index email data by date\n",
    "#     df['date'] = pd.to_datetime(df['date'])\n",
    "#     df.set_index('date', inplace=True)\n",
    "\n",
    "#     #Fill the missing dates and corresponding feature values as 0.\n",
    "#     df = df.resample('D').sum().fillna(0)\n",
    "\n",
    "#     #assuming baseline_x is date indexed and missed dates already filled\n",
    "#     X = baseline_df[feature_name] #debug_df['date'] # data_as_frame['Months']\n",
    "#     Y = df[feature_name] #data_as_frame['SunSpots']\n",
    "\n",
    "#     # # plot the results\n",
    "#     # plot_results(X, y=Y, window_size=20, text_xlabel=\"Months\", sigma_value=3, text_ylabel=\"No. of Sun spots\")\n",
    "\n",
    "#     events = explain_anomalies_cmnty(X, Y, window_size=ws, sigma=sig)\n",
    "#     # Display the anomaly dict\n",
    "#     print(\"Information about the anomalies model:{}\".format(events))\n",
    "#     print()\n",
    "\n",
    "#     events_rolling = explain_anomalies_rolling_std_cmnty(X, Y, window_size=ws, sigma=sig)\n",
    "#     # Display the anomaly dict\n",
    "#     print(\"Information about the anomalies model rolling:{}\".format(events_rolling))\n",
    "#     print()\n",
    "\n",
    "#     #####################################################################################################\n",
    "\n",
    "#     plot_res_cmnty (events, X, Y, baseline_type='PBP', feature_type=feature_name, user_name=uname)\n",
    "#     # plot_res (events_rolling, Y, baseline_type='UBP-rolling', feature_type=feature_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#now df is ready to groupby [user, date], [cmnty, date], [peer,date]\n",
    "########email_freq_feature_dic = prep_ubp_dic(email_freq_feature_df)\n",
    "email_freq_feature_dic = prep_ubp_dic(email_freq_feature_df)\n",
    "\n",
    "email_freq_feature_peer_dic = prep_pbp_dic(email_freq_feature_peer_df)\n",
    "\n",
    "email_freq_feature_cmnty_dic = prep_cbp_dic(email_freq_feature_cmnty_df)\n",
    "\n",
    "\n",
    "pickle_store(\"pickle/f1/f1_ubp_file\", email_freq_feature_dic) #raw means not indexed by Timeindex\n",
    "pickle_store(\"pickle/f1/f1_pbp_file\", email_freq_feature_peer_dic) #raw means not indexed by Timeindex\n",
    "pickle_store(\"pickle/f1/f1_cbp_file\", email_freq_feature_cmnty_dic) #raw means not indexed by Timeindex\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_freq_feature_dic = pickle_restore(\"pickle/f1/f1_ubp_file\") #raw means not indexed by Timeindex\n",
    "email_freq_feature_peer_dic = pickle_restore(\"pickle/f1/f1_pbp_file\") #raw means not indexed by Timeindex\n",
    "email_freq_feature_cmnty_dic = pickle_restore(\"pickle/f1/f1_cbp_file\") #raw means not indexed by Timeindex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "print('PBP')\n",
    "pbp_events_dic={}\n",
    "for key in email_freq_feature_dic:\n",
    "    usr=key #'AAM0658'\n",
    "    debug_df = email_freq_feature_dic[usr]\n",
    "    debug_peer_df = email_freq_feature_peer_dic[usr_peer_map[usr]]\n",
    "\n",
    "    #do a depp copy of preped feature dataframe\n",
    "    df = debug_df.copy()\n",
    "    df_peer = debug_peer_df.copy()\n",
    "    feature_name = 'email_freq'\n",
    "    uname = key\n",
    "    anom_ev=anom_calc_pbp(df, df_peer, feature_name, uname, ws=10, sig=3)\n",
    "\n",
    "    if len(anom_ev['anomalies_dict']):\n",
    "        pbp_events_dic[key]=anom_ev\n",
    "    \n",
    "    # count += 1\n",
    "    # if count%5 == 0:\n",
    "    #     break\n",
    "print('CBP')\n",
    "cbp_events_dic={}\n",
    "for key in email_freq_feature_dic:\n",
    "    usr=key\n",
    "    debug_df = email_freq_feature_dic[usr]\n",
    "    debug_cmnty_df = email_freq_feature_cmnty_dic[usr_cmnty_map[usr]]\n",
    "\n",
    "    #do a depp copy of preped feature dataframe\n",
    "    df = debug_df.copy()\n",
    "    df_cmnty = debug_cmnty_df.copy()\n",
    "\n",
    "    feature_name = 'email_freq'\n",
    "    uname = key\n",
    "    anom_ev=anom_calc_cbp(df, df_cmnty, feature_name, uname, ws=10, sig=3)\n",
    "\n",
    "    if len(anom_ev['anomalies_dict']):\n",
    "        cbp_events_dic[key]=anom_ev\n",
    "\n",
    "    \n",
    "    # count += 1\n",
    "    # if count%5 == 0:\n",
    "    #     break\n",
    "\n",
    "print('UBP')\n",
    "ubp_events_dic={}\n",
    "# email_freq_feature_dic.keys()\n",
    "# Feature data format:  user: df [user, date, featurecolmn]\n",
    "for key in email_freq_feature_dic:\n",
    "    usr=key\n",
    "    debug_df = email_freq_feature_dic[usr]\n",
    "\n",
    "\n",
    "    #do a depp copy of preped feature dataframe\n",
    "    df = debug_df.copy()\n",
    "    anom_ev = anom_calc_ubp(df, 'email_freq', usr, ws=10, sig=3)\n",
    "\n",
    "    if len(anom_ev['anomalies_dict']):\n",
    "        ubp_events_dic[key]=anom_ev\n",
    "\n",
    "    \n",
    "    # count += 1\n",
    "    # if count%100 == 0:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#post process anomalies: delete users with 0 email freq.\n",
    "#do this to ensure that anom with 0 email freq value are trimmed as they are false positives. \n",
    "f1_anom_user_lst=[]\n",
    "# for anom_usr in ubp_events_dic:\n",
    "#     a_ev = ubp_events_dic[anom_usr]\n",
    "#     print('dict a_ev=', a_ev['anomalies_dict'])\n",
    "#     for k, v in a_ev['anomalies_dict'].items():\n",
    "#         if v != 0:\n",
    "#             f1_anom_user_lst.append(anom_usr)\n",
    "#             break\n",
    "\n",
    "f1_anom_user_lst += get_anom_list(ubp_events_dic)\n",
    "f1_anom_user_lst += get_anom_list(pbp_events_dic)\n",
    "f1_anom_user_lst += get_anom_list(cbp_events_dic)\n",
    "\n",
    "\n",
    "# for anom_usr in pbp_events_dic:\n",
    "#     a_ev = pbp_events_dic[anom_usr]\n",
    "#     print('dict a_ev=', a_ev['anomalies_dict'])\n",
    "#     for k, v in a_ev['anomalies_dict'].items():\n",
    "#         if v != 0:\n",
    "#             f1_anom_user_lst.append(anom_usr)\n",
    "#             break\n",
    "\n",
    "# for anom_usr in cbp_events_dic:\n",
    "#     a_ev = cbp_events_dic[anom_usr]\n",
    "#     print('dict a_ev=', a_ev['anomalies_dict'])\n",
    "#     for k, v in a_ev['anomalies_dict'].items():\n",
    "#         if v != 0:\n",
    "#             f1_anom_user_lst.append(anom_usr)\n",
    "#             break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(f1_anom_user_lst))\n",
    "\n",
    "# save list of anom users\n",
    "pickle_store(\"pickle/f1/f1_anom_set_file\", set(f1_anom_user_lst)) #raw means not indexed by Timeindex\n"
   ]
  }
 ]
}
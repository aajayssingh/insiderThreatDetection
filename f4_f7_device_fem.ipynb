{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "from helper import pickle_store, pickle_restore\n",
    "import dateutil\n",
    "from adem import *\n",
    "from collections import Counter \n",
    "\n",
    "\n",
    "#fetch all rawdata and save as pickle format for speed.\n",
    "dev_raw_df = pd.read_csv(\"r4.2/device.csv\") # 2629979 rows, 11 columns\n",
    "# Select small data till 1 April\n",
    "pickle_store(\"pickle/dev_rawdf_file\", dev_raw_df) #raw means not indexed by Timeindex\n",
    "\n",
    "#fetch all rawdata and save as pickle format for speed.\n",
    "log_raw_df = pd.read_csv(\"r4.2/logon.csv\") # 2629979 rows, 11 columns\n",
    "# Select small data till 1 April\n",
    "pickle_store(\"pickle/log_rawdf_file\", log_raw_df) #raw means not indexed by Timeindex\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "isdev=True  #f4 true, f7false \n",
    "\n",
    "#get email data of first 3 months: data is without date column converted to timedates type\n",
    "if isdev:\n",
    "    dev_raw_df = pickle_restore(\"pickle/dev_rawdf_file\")\n",
    "else:\n",
    "    dev_raw_df = pickle_restore(\"pickle/log_rawdf_file\")\n",
    "\n",
    "dev_act_feature_df=dev_raw_df[['user', 'date', 'pc', 'activity']]\n",
    "\n",
    "\n",
    "dev_act_feature_df['date'] = pd.to_datetime(dev_act_feature_df['date'])\n",
    "# mask = dev_act_feature_df['date'] <= end_d\n",
    "mask = (dev_act_feature_df['date'] <= end_d) & (dev_act_feature_df['date'] >= start_d)\n",
    "dev_act_feature_df = dev_act_feature_df.loc[mask]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#create a copy of orig df so that I can separately process for ubp pbp and cbp\n",
    "dev_act_feature_cmnty_df = dev_act_feature_df.copy()\n",
    "dev_act_feature_peer_df = dev_act_feature_df.copy()\n",
    "\n",
    "# 1. in email_freq_feature_df add community and peer column with id as values.\n",
    "usr_cmnty_map = pickle_restore(\"pickle/community_louvian_file\")\n",
    "dev_act_feature_cmnty_df['cmnty']=dev_act_feature_cmnty_df.apply(lambda row: usr_cmnty_map[row.user], axis=1)\n",
    "\n",
    "usr_peer_map = pickle_restore(\"pickle/eid_role_map_file\")\n",
    "dev_act_feature_peer_df['peer']=dev_act_feature_peer_df.apply(lambda row: usr_peer_map[row.user], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           user                date       pc    activity\n",
       "131068  EIS0041 2010-06-01 00:47:01  PC-3124     Connect\n",
       "131069  BSS0369 2010-06-01 00:47:38  PC-8508     Connect\n",
       "131070  BSS0369 2010-06-01 00:47:58  PC-8508  Disconnect\n",
       "131071  EIS0041 2010-06-01 00:51:34  PC-3124  Disconnect\n",
       "131072  MOS0047 2010-06-01 01:23:00  PC-6688     Connect\n",
       "...         ...                 ...      ...         ...\n",
       "305590  FMG0527 2010-12-30 22:41:33  PC-4256  Disconnect\n",
       "305591  FMG0527 2010-12-30 23:08:43  PC-4256     Connect\n",
       "305592  FMG0527 2010-12-30 23:12:54  PC-4256  Disconnect\n",
       "305593  FMG0527 2010-12-30 23:34:33  PC-4256     Connect\n",
       "305594  FMG0527 2010-12-30 23:36:44  PC-4256  Disconnect\n",
       "\n",
       "[174527 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user</th>\n      <th>date</th>\n      <th>pc</th>\n      <th>activity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>131068</th>\n      <td>EIS0041</td>\n      <td>2010-06-01 00:47:01</td>\n      <td>PC-3124</td>\n      <td>Connect</td>\n    </tr>\n    <tr>\n      <th>131069</th>\n      <td>BSS0369</td>\n      <td>2010-06-01 00:47:38</td>\n      <td>PC-8508</td>\n      <td>Connect</td>\n    </tr>\n    <tr>\n      <th>131070</th>\n      <td>BSS0369</td>\n      <td>2010-06-01 00:47:58</td>\n      <td>PC-8508</td>\n      <td>Disconnect</td>\n    </tr>\n    <tr>\n      <th>131071</th>\n      <td>EIS0041</td>\n      <td>2010-06-01 00:51:34</td>\n      <td>PC-3124</td>\n      <td>Disconnect</td>\n    </tr>\n    <tr>\n      <th>131072</th>\n      <td>MOS0047</td>\n      <td>2010-06-01 01:23:00</td>\n      <td>PC-6688</td>\n      <td>Connect</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>305590</th>\n      <td>FMG0527</td>\n      <td>2010-12-30 22:41:33</td>\n      <td>PC-4256</td>\n      <td>Disconnect</td>\n    </tr>\n    <tr>\n      <th>305591</th>\n      <td>FMG0527</td>\n      <td>2010-12-30 23:08:43</td>\n      <td>PC-4256</td>\n      <td>Connect</td>\n    </tr>\n    <tr>\n      <th>305592</th>\n      <td>FMG0527</td>\n      <td>2010-12-30 23:12:54</td>\n      <td>PC-4256</td>\n      <td>Disconnect</td>\n    </tr>\n    <tr>\n      <th>305593</th>\n      <td>FMG0527</td>\n      <td>2010-12-30 23:34:33</td>\n      <td>PC-4256</td>\n      <td>Connect</td>\n    </tr>\n    <tr>\n      <th>305594</th>\n      <td>FMG0527</td>\n      <td>2010-12-30 23:36:44</td>\n      <td>PC-4256</td>\n      <td>Disconnect</td>\n    </tr>\n  </tbody>\n</table>\n<p>174527 rows Ã— 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "dev_act_feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #select data only till 1 April (4/1/2010). \n",
    "\n",
    "# # convert date column in date form to comapre and select small data till desired date.\n",
    "# # df = dev_act_feature_df.copy()\n",
    "# dev_act_feature_df['date'] = pd.to_datetime(dev_act_feature_df['date'])\n",
    "# mask = dev_act_feature_df['date'] <= '04-2-2010'\n",
    "# dev_act_feature_df = dev_act_feature_df.loc[mask]\n",
    "\n",
    "\n",
    "# dev_act_feature_cmnty_df['date'] = pd.to_datetime(dev_act_feature_cmnty_df['date'])\n",
    "# mask = dev_act_feature_cmnty_df['date'] <= '04-2-2010'\n",
    "# dev_act_feature_cmnty_df = dev_act_feature_cmnty_df.loc[mask]\n",
    "\n",
    "\n",
    "# dev_act_feature_peer_df['date'] = pd.to_datetime(dev_act_feature_peer_df['date'])\n",
    "# mask = dev_act_feature_peer_df['date'] <= '04-2-2010'\n",
    "# dev_act_feature_peer_df = dev_act_feature_peer_df.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also NOTE that unlike emails I have converted the dates to datetime format. \n",
    "# Convert date back to string format\n",
    "dev_act_feature_df['date']=dev_act_feature_df['date'].astype(str)\n",
    "dev_act_feature_cmnty_df['date']=dev_act_feature_cmnty_df['date'].astype(str)\n",
    "dev_act_feature_peer_df['date']=dev_act_feature_peer_df['date'].astype(str)\n",
    "\n",
    "# dev_act_feature_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now data is ready to generate prep_dic using groupby, UBP. Feature = num_activity\n",
    "def prep_ubp_dic(df):\n",
    "    #convert date column of srt type to Timestamp type.\n",
    "    df['date'] = df['date'].apply(dateutil.parser.parse, dayfirst=False) #NOTE converts day to day first year-dd-mm\n",
    "    #now convert date column of Timestamptype to only date values and remove hr:mm:ss\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "\n",
    "    #creates user, date wise groups and each internal group is a dataframe.\n",
    "    # grp=tmp_email_freq_feature_df.groupby(['user', 'date'])\n",
    "\n",
    "    # create user, date wise groups and count unique dates then add a separate column for the counts\n",
    "    df=df.groupby(['user', 'date']).activity.agg('count').to_frame('num_usage').reset_index()\n",
    "\n",
    "    #groupby user now To populate a dictionary for each user.\n",
    "    grp=df.groupby(['user'])\n",
    "\n",
    "    email_freq_feature_dic={} #user:df with feature value\n",
    "    #iterating groups\n",
    "    for name, group in grp:\n",
    "        # print (name)\n",
    "        # print (group)\n",
    "        email_freq_feature_dic[name]=group\n",
    "        # ldf = group.groupby(['date'], as_index=False)['cntr'].size()\n",
    "\n",
    "    return email_freq_feature_dic\n",
    "\n",
    "\n",
    "def prep_cbp_dic(df):\n",
    "    #convert date column of srt type to Timestamp type.\n",
    "    df['date'] = df['date'].apply(dateutil.parser.parse, dayfirst=False)\n",
    "    #now convert date column of Timestamptype to only date values and remove hr:mm:ss\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "\n",
    "    # create user, date wise groups and count unique dates then add a separate column for the counts\n",
    "    df=df.groupby(['cmnty', 'date']).activity.agg('count').to_frame('num_usage').reset_index()\n",
    "\n",
    "    cmnty_len = Counter(usr_cmnty_map.values())\n",
    "    #make values of email freq avg of community\n",
    "    df['num_usage'] = df.apply( lambda x: x.num_usage/cmnty_len[x.cmnty], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    #groupby user now To populate a dictionary for each user.\n",
    "    grp=df.groupby(['cmnty'])\n",
    "\n",
    "    feature_cmnty_dic={} #user:df with feature value\n",
    "    #iterating groups\n",
    "    for name, group in grp:\n",
    "        # print (name)\n",
    "        # print (group)\n",
    "        feature_cmnty_dic[name]=group\n",
    "        # ldf = group.groupby(['date'], as_index=False)['cntr'].size()\n",
    "\n",
    "\n",
    "\n",
    "    return feature_cmnty_dic\n",
    "\n",
    "def prep_pbp_dic(df):\n",
    "    #convert date column of srt type to Timestamp type.\n",
    "    df['date'] = df['date'].apply(dateutil.parser.parse, dayfirst=False)\n",
    "    #now convert date column of Timestamptype to only date values and remove hr:mm:ss\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "\n",
    "    # create user, date wise groups and count unique dates then add a separate column for the counts\n",
    "    df=df.groupby(['peer', 'date']).activity.agg('count').to_frame('num_usage').reset_index()\n",
    "\n",
    "    cmnty_len = Counter(usr_peer_map.values())\n",
    "    #make values of email freq avg of community\n",
    "    df['num_usage'] = df.apply( lambda x: x.num_usage/cmnty_len[x.peer], axis=1)\n",
    "\n",
    "    #groupby user now To populate a dictionary for each user.\n",
    "    grp=df.groupby(['peer'])\n",
    "\n",
    "    feature_peer_dic={} #user:df with feature value\n",
    "    #iterating groups\n",
    "    for name, group in grp:\n",
    "        # print (name)\n",
    "        # print (group)\n",
    "        feature_peer_dic[name]=group\n",
    "        # ldf = group.groupby(['date'], as_index=False)['cntr'].size()\n",
    "    return feature_peer_dic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ubp_dev_act_feature_df = dev_act_feature_df.copy()\n",
    "ubp_dev_act_feature_dic = prep_ubp_dic(ubp_dev_act_feature_df)\n",
    "\n",
    "\n",
    "cbp_dev_act_feature_df = dev_act_feature_cmnty_df.copy()\n",
    "cbp_dev_act_feature_dic = prep_cbp_dic(cbp_dev_act_feature_df)\n",
    "\n",
    "\n",
    "pbp_dev_act_feature_df = dev_act_feature_peer_df.copy()\n",
    "pbp_dev_act_feature_dic = prep_pbp_dic(pbp_dev_act_feature_df)\n",
    "\n",
    "\n",
    "#store all preped dic\n",
    "if isdev:\n",
    "    pickle_store(\"pickle/f4/f4_ubp_file\", ubp_dev_act_feature_dic)\n",
    "    pickle_store(\"pickle/f4/f4_pbp_file\", pbp_dev_act_feature_dic)\n",
    "    pickle_store(\"pickle/f4/f4_cbp_file\", cbp_dev_act_feature_dic)\n",
    "else:\n",
    "    pickle_store(\"pickle/f7/f7_ubp_file\", ubp_dev_act_feature_dic)\n",
    "    pickle_store(\"pickle/f7/f7_pbp_file\", pbp_dev_act_feature_dic)\n",
    "    pickle_store(\"pickle/f7/f7_cbp_file\", cbp_dev_act_feature_dic)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isdev:\n",
    "    ubp_dev_act_feature_dic = pickle_restore(\"pickle/f4/f4_ubp_file\") \n",
    "    pbp_dev_act_feature_dic = pickle_restore(\"pickle/f4/f4_pbp_file\") \n",
    "    cbp_dev_act_feature_dic = pickle_restore(\"pickle/f4/f4_cbp_file\") \n",
    "else:\n",
    "    ubp_dev_act_feature_dic = pickle_restore(\"pickle/f7/f7_ubp_file\") \n",
    "    pbp_dev_act_feature_dic = pickle_restore(\"pickle/f7/f7_pbp_file\") \n",
    "    cbp_dev_act_feature_dic = pickle_restore(\"pickle/f7/f7_cbp_file\") \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "feature_name = 'num_usage'\n",
    "print('PBP')\n",
    "pbp_events_dic={}\n",
    "for key in ubp_dev_act_feature_dic:\n",
    "    usr=key #'AAM0658'\n",
    "    debug_df = ubp_dev_act_feature_dic[usr]\n",
    "    debug_peer_df = pbp_dev_act_feature_dic[usr_peer_map[usr]]\n",
    "\n",
    "    #do a depp copy of preped feature dataframe\n",
    "    df = debug_df.copy()\n",
    "    df_peer = debug_peer_df.copy()\n",
    "    \n",
    "    uname = key\n",
    "    print(key)\n",
    "    anom_ev = anom_calc_pbp(df, df_peer, feature_name, uname, ws=10, sig=3)\n",
    "\n",
    "    if len(anom_ev['anomalies_dict']):\n",
    "        pbp_events_dic[key]=anom_ev\n",
    "    \n",
    "    # count += 1\n",
    "    # if count%1 == 0:\n",
    "    #     break\n",
    "\n",
    "print('CBP')\n",
    "cbp_events_dic={}\n",
    "for key in ubp_dev_act_feature_dic:\n",
    "    usr=key\n",
    "    debug_df = ubp_dev_act_feature_dic[usr]\n",
    "    debug_cmnty_df = cbp_dev_act_feature_dic[usr_cmnty_map[usr]]\n",
    "\n",
    "    #do a depp copy of preped feature dataframe\n",
    "    df = debug_df.copy()\n",
    "    df_cmnty = debug_cmnty_df.copy()\n",
    "\n",
    "    uname = key\n",
    "    print(key)\n",
    "    anom_ev = anom_calc_cbp(df, df_cmnty, feature_name, uname, ws=10, sig=3)\n",
    "\n",
    "    if len(anom_ev['anomalies_dict']):\n",
    "        cbp_events_dic[key]=anom_ev\n",
    "    \n",
    "    # count += 1\n",
    "    # if count%1 == 0:\n",
    "    #     break\n",
    "\n",
    "\n",
    "print('UBP')\n",
    "ubp_events_dic={}\n",
    "# email_freq_feature_dic.keys()\n",
    "# Feature data format:  user: df [user, date, featurecolmn]\n",
    "for key in ubp_dev_act_feature_dic:\n",
    "    usr=key\n",
    "    debug_df = ubp_dev_act_feature_dic[usr]\n",
    "\n",
    "\n",
    "    #do a depp copy of preped feature dataframe\n",
    "    df = debug_df.copy()\n",
    "    print(key)\n",
    "    anom_ev = anom_calc_ubp(df, feature_name, usr, ws=10, sig=3)\n",
    "\n",
    "    if len(anom_ev['anomalies_dict']):\n",
    "        ubp_events_dic[key]=anom_ev\n",
    "\n",
    "    # count += 1\n",
    "    # if count%100 == 0:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import get_anom_list\n",
    "\n",
    "anom_user_lst=[]\n",
    "# for anom_usr in ubp_events_dic:\n",
    "#     a_ev = ubp_events_dic[anom_usr]\n",
    "#     print('dict a_ev=', a_ev['anomalies_dict'])\n",
    "#     for k, v in a_ev['anomalies_dict'].items():\n",
    "#         if v != 0:\n",
    "#             f1_anom_user_lst.append(anom_usr)\n",
    "#             break\n",
    "\n",
    "anom_user_lst += get_anom_list(ubp_events_dic)\n",
    "anom_user_lst += get_anom_list(pbp_events_dic)\n",
    "anom_user_lst += get_anom_list(cbp_events_dic)\n",
    "\n",
    "len(set(anom_user_lst))\n",
    "\n",
    "# save list of anom users\n",
    "if isdev:\n",
    "    pickle_store(\"pickle/f4/f4_anom_set_file\", set(anom_user_lst)) #raw means not indexed by Timeindex\n",
    "else:\n",
    "    pickle_store(\"pickle/f7/f7_anom_set_file\", set(anom_user_lst)) #raw means not indexed by Timeindex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(anom_user_lst))\n",
    "\n",
    "set(anom_user_lst)"
   ]
  }
 ]
}
{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "from helper import pickle_store, pickle_restore\n",
    "import dateutil\n",
    "from adem import *\n",
    "from collections import Counter \n",
    "\n",
    "\n",
    "# #fetch all rawdata and save as pickle format for speed.\n",
    "# dev_raw_df = pd.read_csv(\"r4.2/device.csv\") # 2629979 rows, 11 columns\n",
    "\n",
    "# # Select small data till 1 April\n",
    "# pickle_store(\"dev_rawdf_file\", dev_raw_df) #raw means not indexed by Timeindex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get email data of first 3 months: data is without date column converted to timedates type\n",
    "isdev=False #f5 True, F8 False\n",
    "\n",
    "#get email data of first 3 months: data is without date column converted to timedates type\n",
    "if isdev:\n",
    "    dev_raw_df = pickle_restore(\"pickle/dev_rawdf_file\")\n",
    "else:\n",
    "    dev_raw_df = pickle_restore(\"pickle/log_rawdf_file\")\n",
    "\n",
    "# dev_raw_df = pickle_restore(\"dev_rawdf_file\")\n",
    "\n",
    "dev_act_feature_df=dev_raw_df[['user', 'date', 'pc']] #ONLY CHANGE iS THIS wrt device_fem\n",
    "\n",
    "\n",
    "dev_act_feature_df['date'] = pd.to_datetime(dev_act_feature_df['date'])\n",
    "# mask = dev_act_feature_df['date'] <= end_d\n",
    "mask = (dev_act_feature_df['date'] <= end_d) & (dev_act_feature_df['date'] >= start_d)\n",
    "dev_act_feature_df = dev_act_feature_df.loc[mask]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#create a copy of orig df so that I can separately process for ubp pbp and cbp\n",
    "dev_act_feature_cmnty_df = dev_act_feature_df.copy()\n",
    "dev_act_feature_peer_df = dev_act_feature_df.copy()\n",
    "\n",
    "# 1. in email_freq_feature_df add community and peer column with id as values.\n",
    "usr_cmnty_map = pickle_restore(\"pickle/community_louvian_file\")\n",
    "dev_act_feature_cmnty_df['cmnty']=dev_act_feature_cmnty_df.apply(lambda row: usr_cmnty_map[row.user], axis=1)\n",
    "\n",
    "usr_peer_map = pickle_restore(\"pickle/eid_role_map_file\")\n",
    "dev_act_feature_peer_df['peer']=dev_act_feature_peer_df.apply(lambda row: usr_peer_map[row.user], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           user                date       pc\n",
       "275257  JAV0361 2010-06-01 00:03:01  PC-8149\n",
       "275258  TSM0363 2010-06-01 00:09:22  PC-7145\n",
       "275259  TSM0363 2010-06-01 00:13:29  PC-7145\n",
       "275260  JAV0361 2010-06-01 00:14:16  PC-4600\n",
       "275261  JAV0361 2010-06-01 00:18:59  PC-8149\n",
       "...         ...                 ...      ...\n",
       "640454  WDD0366 2010-12-30 23:27:39  PC-8148\n",
       "640455  MSH0040 2010-12-30 23:31:50  PC-3686\n",
       "640456  WDD0366 2010-12-30 23:35:45  PC-8148\n",
       "640457  TSM0363 2010-12-30 23:48:34  PC-1845\n",
       "640458  TSM0363 2010-12-30 23:53:15  PC-1845\n",
       "\n",
       "[365202 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user</th>\n      <th>date</th>\n      <th>pc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>275257</th>\n      <td>JAV0361</td>\n      <td>2010-06-01 00:03:01</td>\n      <td>PC-8149</td>\n    </tr>\n    <tr>\n      <th>275258</th>\n      <td>TSM0363</td>\n      <td>2010-06-01 00:09:22</td>\n      <td>PC-7145</td>\n    </tr>\n    <tr>\n      <th>275259</th>\n      <td>TSM0363</td>\n      <td>2010-06-01 00:13:29</td>\n      <td>PC-7145</td>\n    </tr>\n    <tr>\n      <th>275260</th>\n      <td>JAV0361</td>\n      <td>2010-06-01 00:14:16</td>\n      <td>PC-4600</td>\n    </tr>\n    <tr>\n      <th>275261</th>\n      <td>JAV0361</td>\n      <td>2010-06-01 00:18:59</td>\n      <td>PC-8149</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>640454</th>\n      <td>WDD0366</td>\n      <td>2010-12-30 23:27:39</td>\n      <td>PC-8148</td>\n    </tr>\n    <tr>\n      <th>640455</th>\n      <td>MSH0040</td>\n      <td>2010-12-30 23:31:50</td>\n      <td>PC-3686</td>\n    </tr>\n    <tr>\n      <th>640456</th>\n      <td>WDD0366</td>\n      <td>2010-12-30 23:35:45</td>\n      <td>PC-8148</td>\n    </tr>\n    <tr>\n      <th>640457</th>\n      <td>TSM0363</td>\n      <td>2010-12-30 23:48:34</td>\n      <td>PC-1845</td>\n    </tr>\n    <tr>\n      <th>640458</th>\n      <td>TSM0363</td>\n      <td>2010-12-30 23:53:15</td>\n      <td>PC-1845</td>\n    </tr>\n  </tbody>\n</table>\n<p>365202 rows Ã— 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "dev_act_feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #select data only till 1 April (4/1/2010). \n",
    "\n",
    "# # convert date column in date form to comapre and select small data till desired date.\n",
    "# # df = dev_act_feature_df.copy()\n",
    "# dev_act_feature_df['date'] = pd.to_datetime(dev_act_feature_df['date'])\n",
    "# mask = dev_act_feature_df['date'] <= '04-2-2010'\n",
    "# dev_act_feature_df = dev_act_feature_df.loc[mask]\n",
    "\n",
    "\n",
    "# dev_act_feature_cmnty_df['date'] = pd.to_datetime(dev_act_feature_cmnty_df['date'])\n",
    "# mask = dev_act_feature_cmnty_df['date'] <= '04-2-2010'\n",
    "# dev_act_feature_cmnty_df = dev_act_feature_cmnty_df.loc[mask]\n",
    "\n",
    "\n",
    "# dev_act_feature_peer_df['date'] = pd.to_datetime(dev_act_feature_peer_df['date'])\n",
    "# mask = dev_act_feature_peer_df['date'] <= '04-2-2010'\n",
    "# dev_act_feature_peer_df = dev_act_feature_peer_df.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also NOTE that unlike emails I have converted the dates to datetime format. \n",
    "# Convert date back to string format\n",
    "dev_act_feature_df['date']=dev_act_feature_df['date'].astype(str)\n",
    "dev_act_feature_cmnty_df['date']=dev_act_feature_cmnty_df['date'].astype(str)\n",
    "dev_act_feature_peer_df['date']=dev_act_feature_peer_df['date'].astype(str)\n",
    "\n",
    "# dev_act_feature_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now data is ready to generate prep_dic using groupby, UBP. Feature = num_activity\n",
    "def prep_ubp_dic(df):\n",
    "    #convert date column of srt type to Timestamp type.\n",
    "    df['date'] = df['date'].apply(dateutil.parser.parse, dayfirst=False) #NOTE converts day to day first year-dd-mm\n",
    "    #now convert date column of Timestamptype to only date values and remove hr:mm:ss\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "\n",
    "    #creates user, date wise groups and each internal group is a dataframe.\n",
    "    # grp=tmp_email_freq_feature_df.groupby(['user', 'date'])\n",
    "\n",
    "    # create user, date wise groups and count unique dates then add a separate column for the counts\n",
    "    df=df.groupby(['user', 'date']).pc.agg('nunique').to_frame('num_pc').reset_index()\n",
    "\n",
    "    #groupby user now To populate a dictionary for each user.\n",
    "    grp=df.groupby(['user'])\n",
    "\n",
    "    email_freq_feature_dic={} #user:df with feature value\n",
    "    #iterating groups\n",
    "    for name, group in grp:\n",
    "        # print (name)\n",
    "        # print (group)\n",
    "        email_freq_feature_dic[name]=group\n",
    "        # ldf = group.groupby(['date'], as_index=False)['cntr'].size()\n",
    "\n",
    "    return email_freq_feature_dic\n",
    "\n",
    "\n",
    "def prep_cbp_dic(df):\n",
    "    #convert date column of srt type to Timestamp type.\n",
    "    df['date'] = df['date'].apply(dateutil.parser.parse, dayfirst=False)\n",
    "    #now convert date column of Timestamptype to only date values and remove hr:mm:ss\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "\n",
    "    # create user, date wise groups and count unique dates then add a separate column for the counts\n",
    "    df=df.groupby(['cmnty', 'date']).pc.agg('nunique').to_frame('num_pc').reset_index()\n",
    "\n",
    "    cmnty_len = Counter(usr_cmnty_map.values())\n",
    "    #make values of email freq avg of community\n",
    "    df['num_pc'] = df.apply( lambda x: x.num_pc/cmnty_len[x.cmnty], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    #groupby user now To populate a dictionary for each user.\n",
    "    grp=df.groupby(['cmnty'])\n",
    "\n",
    "    feature_cmnty_dic={} #user:df with feature value\n",
    "    #iterating groups\n",
    "    for name, group in grp:\n",
    "        # print (name)\n",
    "        # print (group)\n",
    "        feature_cmnty_dic[name]=group\n",
    "        # ldf = group.groupby(['date'], as_index=False)['cntr'].size()\n",
    "\n",
    "\n",
    "\n",
    "    return feature_cmnty_dic\n",
    "\n",
    "def prep_pbp_dic(df):\n",
    "    #convert date column of srt type to Timestamp type.\n",
    "    df['date'] = df['date'].apply(dateutil.parser.parse, dayfirst=False)\n",
    "    #now convert date column of Timestamptype to only date values and remove hr:mm:ss\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "\n",
    "    # create user, date wise groups and count unique dates then add a separate column for the counts\n",
    "    df=df.groupby(['peer', 'date']).pc.agg('nunique').to_frame('num_pc').reset_index()\n",
    "\n",
    "    cmnty_len = Counter(usr_peer_map.values())\n",
    "    #make values of email freq avg of community\n",
    "    df['num_pc'] = df.apply( lambda x: x.num_pc/cmnty_len[x.peer], axis=1)\n",
    "\n",
    "    #groupby user now To populate a dictionary for each user.\n",
    "    grp=df.groupby(['peer'])\n",
    "\n",
    "    feature_peer_dic={} #user:df with feature value\n",
    "    #iterating groups\n",
    "    for name, group in grp:\n",
    "        # print (name)\n",
    "        # print (group)\n",
    "        feature_peer_dic[name]=group\n",
    "        # ldf = group.groupby(['date'], as_index=False)['cntr'].size()\n",
    "    return feature_peer_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ubp_dev_act_feature_df = dev_act_feature_df.copy()\n",
    "ubp_dev_act_feature_dic = prep_ubp_dic(ubp_dev_act_feature_df)\n",
    "\n",
    "\n",
    "cbp_dev_act_feature_df = dev_act_feature_cmnty_df.copy()\n",
    "cbp_dev_act_feature_dic = prep_cbp_dic(cbp_dev_act_feature_df)\n",
    "\n",
    "\n",
    "pbp_dev_act_feature_df = dev_act_feature_peer_df.copy()\n",
    "pbp_dev_act_feature_dic = prep_pbp_dic(pbp_dev_act_feature_df)\n",
    "\n",
    "\n",
    "#store all preped dic\n",
    "if isdev:\n",
    "    pickle_store(\"pickle/f5/f5_ubp_file\", ubp_dev_act_feature_dic)\n",
    "    pickle_store(\"pickle/f5/f5_pbp_file\", pbp_dev_act_feature_dic)\n",
    "    pickle_store(\"pickle/f5/f5_cbp_file\", cbp_dev_act_feature_dic)\n",
    "else:\n",
    "    pickle_store(\"pickle/f8/f8_ubp_file\", ubp_dev_act_feature_dic)\n",
    "    pickle_store(\"pickle/f8/f8_pbp_file\", pbp_dev_act_feature_dic)\n",
    "    pickle_store(\"pickle/f8/f8_cbp_file\", cbp_dev_act_feature_dic)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isdev:\n",
    "    ubp_dev_act_feature_dic = pickle_restore(\"pickle/f5/f5_ubp_file\") \n",
    "    pbp_dev_act_feature_dic = pickle_restore(\"pickle/f5/f5_pbp_file\") \n",
    "    cbp_dev_act_feature_dic = pickle_restore(\"pickle/f5/f5_cbp_file\") \n",
    "else:\n",
    "    ubp_dev_act_feature_dic = pickle_restore(\"pickle/f8/f8_ubp_file\") \n",
    "    pbp_dev_act_feature_dic = pickle_restore(\"pickle/f8/f8_pbp_file\") \n",
    "    cbp_dev_act_feature_dic = pickle_restore(\"pickle/f8/f8_cbp_file\") \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adem import *\n",
    "count = 0\n",
    "feature_name = 'num_pc'\n",
    "\n",
    "print(\"PBP\")\n",
    "\n",
    "for key in ubp_dev_act_feature_dic:\n",
    "    usr=key #'AAM0658'\n",
    "    debug_df = ubp_dev_act_feature_dic[usr]\n",
    "    debug_peer_df = pbp_dev_act_feature_dic[usr_peer_map[usr]]\n",
    "\n",
    "    #do a depp copy of preped feature dataframe\n",
    "    df = debug_df.copy()\n",
    "    df_peer = debug_peer_df.copy()\n",
    "    \n",
    "    uname = key\n",
    "    print(key)\n",
    "    anom_calc_pbp(df, df_peer, feature_name, uname, ws=10, sig=3)\n",
    "    \n",
    "    count += 1\n",
    "    if count%100 == 0:\n",
    "        break\n",
    "\n",
    "print(\"CBP\")\n",
    "\n",
    "for key in ubp_dev_act_feature_dic:\n",
    "    usr=key\n",
    "    debug_df = ubp_dev_act_feature_dic[usr]\n",
    "    debug_cmnty_df = cbp_dev_act_feature_dic[usr_cmnty_map[usr]]\n",
    "\n",
    "    #do a depp copy of preped feature dataframe\n",
    "    df = debug_df.copy()\n",
    "    df_cmnty = debug_cmnty_df.copy()\n",
    "\n",
    "    uname = key\n",
    "    print(key)\n",
    "    anom_calc_cbp(df, df_cmnty, feature_name, uname, ws=10, sig=3)\n",
    "    \n",
    "    count += 1\n",
    "    if count%100 == 0:\n",
    "        break\n",
    "\n",
    "print(\"UBP\")\n",
    "\n",
    "# email_freq_feature_dic.keys()\n",
    "# Feature data format:  user: df [user, date, featurecolmn]\n",
    "for key in ubp_dev_act_feature_dic:\n",
    "    usr=key\n",
    "    debug_df = ubp_dev_act_feature_dic[usr]\n",
    "\n",
    "\n",
    "    #do a depp copy of preped feature dataframe\n",
    "    df = debug_df.copy()\n",
    "    print(key)\n",
    "    anom_calc_ubp(df, feature_name, usr, ws=10, sig=3)\n",
    "    \n",
    "    count += 1\n",
    "    if count%100 == 0:\n",
    "        break"
   ]
  }
 ]
}
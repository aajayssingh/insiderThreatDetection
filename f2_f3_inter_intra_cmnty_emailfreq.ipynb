{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "from helper import pickle_store, pickle_restore\n",
    "import dateutil\n",
    "from adem import *\n",
    "from collections import Counter \n",
    "\n",
    "\n",
    "# restore picke which was stored in email_fem.ipynb\n",
    "#get email data of first 3 months: data is without date column converted to timedates type\n",
    "email_raw_df = pickle_restore(\"pickle/email_rawdf_file\")\n",
    "usr_cmnty_map = pickle_restore(\"pickle/community_louvian_file\")\n",
    "usr_peer_map = pickle_restore(\"pickle/eid_role_map_file\")\n",
    "email_eid_map = pickle_restore(\"pickle/email_eid_map_file\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_intercmnty_num(x):\n",
    "    user=x.user\n",
    "    receipient_list=[]\n",
    "    if (type(x.to) == str):\n",
    "        # print (x.to.split(';'))\n",
    "        receipient_list+=x.to.split(';')\n",
    "\n",
    "    if (type(x.bcc) == str):\n",
    "        # print (x.bcc.split(';'))\n",
    "        receipient_list+=x.bcc.split(';')    \n",
    "\n",
    "    if (type(x.cc) == str):\n",
    "        # print (x.cc.split(';'))\n",
    "        receipient_list+=x.cc.split(';')        \n",
    "\n",
    "    # print (receipient_list)\n",
    "    inter_cmnty_count=0\n",
    "    intra_cmnty_count=0\n",
    "\n",
    "    for u in receipient_list:\n",
    "        if u in email_eid_map.keys():\n",
    "            if usr_cmnty_map[user] == usr_cmnty_map[email_eid_map[u]]:\n",
    "                intra_cmnty_count+=1\n",
    "            else:\n",
    "                inter_cmnty_count+=1\n",
    "        else:\n",
    "            inter_cmnty_count+=1\n",
    "\n",
    "    # print(inter_cmnty_count, intra_cmnty_count)\n",
    "    return inter_cmnty_count\n",
    "\n",
    "\n",
    "def find_intracmnty_num(x):\n",
    "    user=x.user\n",
    "    receipient_list=[]\n",
    "    if (type(x.to) == str):\n",
    "        # print (x.to.split(';'))\n",
    "        receipient_list+=x.to.split(';')\n",
    "\n",
    "    if (type(x.bcc) == str):\n",
    "        # print (x.bcc.split(';'))\n",
    "        receipient_list+=x.bcc.split(';')    \n",
    "\n",
    "    if (type(x.cc) == str):\n",
    "        # print (x.cc.split(';'))\n",
    "        receipient_list+=x.cc.split(';')        \n",
    "\n",
    "    # print (receipient_list)\n",
    "    inter_cmnty_count=0\n",
    "    intra_cmnty_count=0\n",
    "\n",
    "    for u in receipient_list:\n",
    "        if u in email_eid_map.keys():\n",
    "            if usr_cmnty_map[user] == usr_cmnty_map[email_eid_map[u]]:\n",
    "                intra_cmnty_count+=1\n",
    "            else:\n",
    "                inter_cmnty_count+=1\n",
    "        else:\n",
    "            inter_cmnty_count+=1\n",
    "\n",
    "    # print(inter_cmnty_count, intra_cmnty_count)\n",
    "    return intra_cmnty_count\n",
    "\n",
    "#get email_raw_df with intercmnty_freq and intracmnty_freq columns\n",
    "email_raw_df['intercmnty_freq']=email_raw_df.apply(lambda x: find_intercmnty_num(x), axis = 1)   \n",
    "email_raw_df['intracmnty_freq']=email_raw_df.apply(lambda x: find_intracmnty_num(x), axis = 1)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "done heavy part...\n"
     ]
    }
   ],
   "source": [
    "email_freq_feature_df=email_raw_df[['user', 'date', 'intercmnty_freq', 'intracmnty_freq']]\n",
    "\n",
    "email_freq_feature_df['date'] = pd.to_datetime(email_freq_feature_df['date'])\n",
    "mask = (email_freq_feature_df['date'] <= end_d) & (email_freq_feature_df['date'] >= start_d)\n",
    "email_freq_feature_df = email_freq_feature_df.loc[mask]\n",
    "\n",
    "print(\"done heavy part...\")\n",
    "\n",
    "\n",
    "\n",
    "#create a copy of orig df to operate on deep copied dataframe for UBP , PBP and CBP\n",
    "email_freq_feature_cmnty_df = email_freq_feature_df.copy()\n",
    "email_freq_feature_peer_df = email_freq_feature_df.copy()\n",
    "\n",
    "# 1. in email_freq_feature_df add community and peer column with id as values.\n",
    "email_freq_feature_cmnty_df['cmnty']=email_freq_feature_cmnty_df.apply(lambda row: usr_cmnty_map[row.user], axis=1)\n",
    "\n",
    "email_freq_feature_peer_df['peer']=email_freq_feature_peer_df.apply(lambda row: usr_peer_map[row.user], axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date back to string format\n",
    "email_freq_feature_df['date']=email_freq_feature_df['date'].astype(str)\n",
    "email_freq_feature_cmnty_df['date']=email_freq_feature_cmnty_df['date'].astype(str)\n",
    "email_freq_feature_peer_df['date']=email_freq_feature_peer_df['date'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now the df of form email_raw_df[['user', 'date', 'intercmnty_freq', 'intracmnty_freq']] is ready use groupby to prepare dic for anomaly detection.inter_cmnty_count\n",
    "#heavy operations\n",
    "def prep_ubp_dic(df, inter_cmnty= True):\n",
    "    #convert date column of srt type to Timestamp type.\n",
    "    df['date'] = df['date'].apply(dateutil.parser.parse, dayfirst=True)\n",
    "    #now convert date column of Timestamptype to only date values and remove hr:mm:ss\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "\n",
    "    #creates user, date wise groups and each internal group is a dataframe.\n",
    "    # grp=tmp_email_freq_feature_df.groupby(['user', 'date'])\n",
    "\n",
    "    # create user, date wise groups and count unique dates then add a separate column for the counts\n",
    "    if inter_cmnty:\n",
    "        df=df.groupby(['user', 'date']).intercmnty_freq.agg('sum').to_frame('outcmnty_ef').reset_index()\n",
    "    else:\n",
    "        df=df.groupby(['user', 'date']).intracmnty_freq.agg('sum').to_frame('incmnty_ef').reset_index()\n",
    "\n",
    "    #groupby user now To populate a dictionary for each user.\n",
    "    grp=df.groupby(['user'])\n",
    "\n",
    "    email_freq_feature_dic={} #user:df with feature value\n",
    "    #iterating groups\n",
    "    for name, group in grp:\n",
    "        # print (name)\n",
    "        # print (group)\n",
    "        email_freq_feature_dic[name]=group\n",
    "        # ldf = group.groupby(['date'], as_index=False)['cntr'].size()\n",
    "\n",
    "    return email_freq_feature_dic\n",
    "\n",
    "intercmnty_df = email_freq_feature_df.copy()\n",
    "intracmnty_df = email_freq_feature_df.copy()\n",
    "\n",
    "ef_ubp_intercmnty_feature_dic = prep_ubp_dic(intercmnty_df, inter_cmnty= True)\n",
    "ef_ubp_intracmnty_feature_dic = prep_ubp_dic(intracmnty_df, inter_cmnty= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_cbp_dic(df, inter_cmnty= True):\n",
    "    #convert date column of srt type to Timestamp type.\n",
    "    df['date'] = df['date'].apply(dateutil.parser.parse, dayfirst=True)\n",
    "    #now convert date column of Timestamptype to only date values and remove hr:mm:ss\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "\n",
    "    #creates user, date wise groups and each internal group is a dataframe.\n",
    "    # grp=tmp_email_freq_feature_df.groupby(['user', 'date'])\n",
    "\n",
    "    # create user, date wise groups and count unique dates then add a separate column for the counts\n",
    "    if inter_cmnty:\n",
    "        df=df.groupby(['cmnty', 'date']).intercmnty_freq.agg('sum').to_frame('outcmnty_ef').reset_index()\n",
    "        cmnty_len = Counter(usr_cmnty_map.values())\n",
    "        #make values of email freq avg of community\n",
    "        df['outcmnty_ef'] = df.apply( lambda x: x.outcmnty_ef/cmnty_len[x.cmnty], axis=1)\n",
    "    else:\n",
    "        df=df.groupby(['cmnty', 'date']).intracmnty_freq.agg('sum').to_frame('incmnty_ef').reset_index()\n",
    "        cmnty_len = Counter(usr_cmnty_map.values())\n",
    "        #make values of email freq avg of community\n",
    "        df['incmnty_ef'] = df.apply( lambda x: x.incmnty_ef/cmnty_len[x.cmnty], axis=1)\n",
    "\n",
    "\n",
    "    #groupby user now To populate a dictionary for each user.\n",
    "    grp=df.groupby(['cmnty'])\n",
    "\n",
    "    ef_feature_dic={} #user:df with feature value\n",
    "    #iterating groups\n",
    "    for name, group in grp:\n",
    "        # print (name)\n",
    "        # print (group)\n",
    "        ef_feature_dic[name]=group\n",
    "        # ldf = group.groupby(['date'], as_index=False)['cntr'].size()\n",
    "\n",
    "    return ef_feature_dic    \n",
    "\n",
    "intercmnty_cbp_df = email_freq_feature_cmnty_df.copy()\n",
    "intracmnty_cbp_df = email_freq_feature_cmnty_df.copy()\n",
    "\n",
    "ef_cbp_intercmnty_feature_dic = prep_cbp_dic(intercmnty_cbp_df, inter_cmnty= True)\n",
    "ef_cbp_intracmnty_feature_dic = prep_cbp_dic(intracmnty_cbp_df, inter_cmnty= False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_pbp_dic(df, inter_cmnty= True):\n",
    "    #convert date column of srt type to Timestamp type.\n",
    "    df['date'] = df['date'].apply(dateutil.parser.parse, dayfirst=True)\n",
    "    #now convert date column of Timestamptype to only date values and remove hr:mm:ss\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "\n",
    "    #creates user, date wise groups and each internal group is a dataframe.\n",
    "    # grp=tmp_email_freq_feature_df.groupby(['user', 'date'])\n",
    "\n",
    "    # create user, date wise groups and count unique dates then add a separate column for the counts\n",
    "    if inter_cmnty:\n",
    "        df=df.groupby(['peer', 'date']).intercmnty_freq.agg('sum').to_frame('outcmnty_ef').reset_index()\n",
    "        peer_len = Counter(usr_peer_map.values())\n",
    "        #make values of email freq avg of community\n",
    "        df['outcmnty_ef'] = df.apply( lambda x: x.outcmnty_ef/peer_len[x.peer], axis=1)\n",
    "    else:\n",
    "        df=df.groupby(['peer', 'date']).intracmnty_freq.agg('sum').to_frame('incmnty_ef').reset_index()\n",
    "        peer_len = Counter(usr_peer_map.values())\n",
    "        #make values of email freq avg of community\n",
    "        df['incmnty_ef'] = df.apply( lambda x: x.incmnty_ef/peer_len[x.peer], axis=1)\n",
    "\n",
    "\n",
    "    #groupby user now To populate a dictionary for each user.\n",
    "    grp=df.groupby(['peer'])\n",
    "\n",
    "    ef_feature_dic={} #user:df with feature value\n",
    "    #iterating groups\n",
    "    for name, group in grp:\n",
    "        # print (name)\n",
    "        # print (group)\n",
    "        ef_feature_dic[name]=group\n",
    "        # ldf = group.groupby(['date'], as_index=False)['cntr'].size()\n",
    "\n",
    "    return ef_feature_dic    \n",
    "\n",
    "intercmnty_pbp_df = email_freq_feature_peer_df.copy()\n",
    "intracmnty_pbp_df = email_freq_feature_peer_df.copy()\n",
    "\n",
    "ef_pbp_intercmnty_feature_dic = prep_pbp_dic(intercmnty_pbp_df, inter_cmnty= True)\n",
    "ef_pbp_intracmnty_feature_dic = prep_pbp_dic(intracmnty_pbp_df, inter_cmnty= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ef_ubp_intercmnty_feature_dic = prep_ubp_dic(intercmnty_df, inter_cmnty= True)\n",
    "# ef_ubp_intracmnty_feature_dic = prep_ubp_dic(intracmnty_df, inter_cmnty= False)\n",
    "\n",
    "# ef_cbp_intercmnty_feature_dic = prep_cbp_dic(intercmnty_cbp_df, inter_cmnty= True)\n",
    "# ef_cbp_intracmnty_feature_dic = prep_cbp_dic(intracmnty_cbp_df, inter_cmnty= False)    \n",
    "\n",
    "# ef_pbp_intercmnty_feature_dic = prep_pbp_dic(intercmnty_pbp_df, inter_cmnty= True)\n",
    "# ef_pbp_intracmnty_feature_dic = prep_pbp_dic(intracmnty_pbp_df, inter_cmnty= False)\n",
    "\n",
    "#store all preped dic\n",
    "pickle_store(\"pickle/f2/f2_ubp_file\", ef_ubp_intercmnty_feature_dic) #raw means not indexed by Timeindex\n",
    "pickle_store(\"pickle/f2/f2_pbp_file\", ef_pbp_intercmnty_feature_dic) #raw means not indexed by Timeindex\n",
    "pickle_store(\"pickle/f2/f2_cbp_file\", ef_cbp_intercmnty_feature_dic) #raw means not indexed by Timeindex\n",
    "\n",
    "\n",
    "pickle_store(\"pickle/f3/f3_ubp_file\", ef_ubp_intracmnty_feature_dic) #raw means not indexed by Timeindex\n",
    "pickle_store(\"pickle/f3/f3_pbp_file\", ef_pbp_intracmnty_feature_dic) #raw means not indexed by Timeindex\n",
    "pickle_store(\"pickle/f3/f3_cbp_file\", ef_cbp_intracmnty_feature_dic) #raw means not indexed by Timeindex\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ef_ubp_intercmnty_feature_dic = pickle_restore(\"pickle/f2/f2_ubp_file\") #raw means not indexed by Timeindex\n",
    "ef_pbp_intercmnty_feature_dic = pickle_restore(\"pickle/f2/f2_pbp_file\") #raw means not indexed by Timeindex\n",
    "ef_cbp_intercmnty_feature_dic = pickle_restore(\"pickle/f2/f2_cbp_file\") #raw means not indexed by Timeindex\n",
    "\n",
    "\n",
    "ef_ubp_intracmnty_feature_dic = pickle_restore(\"pickle/f3/f3_ubp_file\") #raw means not indexed by Timeindex\n",
    "ef_pbp_intracmnty_feature_dic = pickle_restore(\"pickle/f3/f3_pbp_file\") #raw means not indexed by Timeindex\n",
    "ef_cbp_intracmnty_feature_dic = pickle_restore(\"pickle/f3/f3_cbp_file\") #raw means not indexed by Timeindex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f2 intercmnty\n",
    "\n",
    "count = 0\n",
    "intercmnty_fname = 'outcmnty_ef'\n",
    "intracmnty_fname = 'incmnty_ef'\n",
    "\n",
    "ubp_events_dic={}\n",
    "for key in ef_ubp_intercmnty_feature_dic:\n",
    "    usr=key\n",
    "    debug_df = ef_ubp_intercmnty_feature_dic[usr]\n",
    "\n",
    "\n",
    "    #do a depp copy of preped feature dataframe\n",
    "    df = debug_df.copy()\n",
    "    anom_ev=anom_calc_ubp(df, intercmnty_fname, usr, ws=10, sig=3)\n",
    "    \n",
    "    if len(anom_ev['anomalies_dict']):\n",
    "        ubp_events_dic[key]=anom_ev\n",
    "    \n",
    "    # count += 1\n",
    "    # if count%10 == 0:\n",
    "    #     break\n",
    "\n",
    "cbp_events_dic={}\n",
    "for key in ef_ubp_intercmnty_feature_dic:\n",
    "    usr=key\n",
    "    debug_df = ef_ubp_intercmnty_feature_dic[usr]\n",
    "    debug_cmnty_df = ef_cbp_intercmnty_feature_dic[usr_cmnty_map[usr]]\n",
    "\n",
    "\n",
    "    #do a depp copy of preped feature dataframe\n",
    "    df = debug_df.copy()\n",
    "    df_cmnty = debug_cmnty_df.copy()\n",
    "\n",
    "    anom_ev=anom_calc_cbp(df, df_cmnty, intercmnty_fname, usr, ws=10, sig=3)\n",
    "\n",
    "    if len(anom_ev['anomalies_dict']):\n",
    "        cbp_events_dic[key]=anom_ev\n",
    "\n",
    "\n",
    "    # count += 1\n",
    "    # if count%10 == 0:\n",
    "    #     break\n",
    "pbp_events_dic={}\n",
    "for key in ef_ubp_intercmnty_feature_dic:\n",
    "    usr=key\n",
    "    debug_df = ef_ubp_intercmnty_feature_dic[usr]\n",
    "    debug_peer_df = ef_pbp_intercmnty_feature_dic[usr_peer_map[usr]]\n",
    "\n",
    "\n",
    "    #do a depp copy of preped feature dataframe\n",
    "    df = debug_df.copy()\n",
    "    df_peer = debug_peer_df.copy()\n",
    "\n",
    "    anom_ev=anom_calc_pbp(df, df_peer, intercmnty_fname, usr, ws=10, sig=3)\n",
    "\n",
    "    if len(anom_ev['anomalies_dict']):\n",
    "        pbp_events_dic[key]=anom_ev\n",
    "    \n",
    "    # count += 1\n",
    "    # if count%10 == 0:\n",
    "    #     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anom_list(events_dic):\n",
    "    anom_user_lst=[]\n",
    "    for anom_usr in events_dic:\n",
    "        a_ev = events_dic[anom_usr]\n",
    "        # print('dict a_ev=', a_ev['anomalies_dict'])\n",
    "        for k, v in a_ev['anomalies_dict'].items():\n",
    "            if v != 0:\n",
    "                anom_user_lst.append(anom_usr)\n",
    "                break\n",
    "    return anom_user_lst\n",
    "\n",
    "\n",
    "f2_anom_user_lst=[]\n",
    "# for anom_usr in ubp_events_dic:\n",
    "#     a_ev = ubp_events_dic[anom_usr]\n",
    "#     print('dict a_ev=', a_ev['anomalies_dict'])\n",
    "#     for k, v in a_ev['anomalies_dict'].items():\n",
    "#         if v != 0:\n",
    "#             f1_anom_user_lst.append(anom_usr)\n",
    "#             break\n",
    "\n",
    "f2_anom_user_lst += get_anom_list(ubp_events_dic)\n",
    "f2_anom_user_lst += get_anom_list(pbp_events_dic)\n",
    "f2_anom_user_lst += get_anom_list(cbp_events_dic)\n",
    "\n",
    "len(set(f2_anom_user_lst))\n",
    "\n",
    "# save list of anom users\n",
    "pickle_store(\"pickle/f2/f2_anom_set_file\", set(f2_anom_user_lst)) #raw means not indexed by Timeindex\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ubp_events_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "######PBP f3 intra\n",
    "\n",
    "count = 0\n",
    "intercmnty_fname = 'outcmnty_ef'\n",
    "intracmnty_fname = 'incmnty_ef'\n",
    "\n",
    "pbp_events_dic={}\n",
    "for key in ef_ubp_intracmnty_feature_dic:\n",
    "    usr=key\n",
    "    debug_df = ef_ubp_intracmnty_feature_dic[usr]\n",
    "    debug_peer_df = ef_pbp_intracmnty_feature_dic[usr_peer_map[usr]]\n",
    "\n",
    "\n",
    "    #do a depp copy of preped feature dataframe\n",
    "    df = debug_df.copy()\n",
    "    df_peer = debug_peer_df.copy()\n",
    "\n",
    "    anom_ev = anom_calc_pbp(df, df_peer, intracmnty_fname, usr, ws=10, sig=3)\n",
    "\n",
    "    if len(anom_ev['anomalies_dict']):\n",
    "        pbp_events_dic[key]=anom_ev\n",
    "\n",
    "    \n",
    "    # count += 1\n",
    "    # if count%20 == 0:\n",
    "    #     break        \n",
    "\n",
    "cbp_events_dic={}\n",
    "for key in ef_ubp_intracmnty_feature_dic:\n",
    "    usr=key\n",
    "    debug_df = ef_ubp_intracmnty_feature_dic[usr]\n",
    "    debug_cmnty_df = ef_cbp_intracmnty_feature_dic[usr_cmnty_map[usr]]\n",
    "\n",
    "\n",
    "    #do a depp copy of preped feature dataframe\n",
    "    df = debug_df.copy()\n",
    "    df_cmnty = debug_cmnty_df.copy()\n",
    "\n",
    "    anom_ev = anom_calc_cbp(df, df_cmnty, intracmnty_fname, usr, ws=10, sig=3)\n",
    "    if len(anom_ev['anomalies_dict']):\n",
    "        cbp_events_dic[key]=anom_ev\n",
    "\n",
    "    \n",
    "    # count += 1\n",
    "    # if count%20 == 0:\n",
    "    #     break        \n",
    "ubp_events_dic={}\n",
    "for key in ef_ubp_intracmnty_feature_dic:\n",
    "    usr=key\n",
    "    debug_df = ef_ubp_intracmnty_feature_dic[usr]\n",
    "\n",
    "\n",
    "    #do a depp copy of preped feature dataframe\n",
    "    df = debug_df.copy()\n",
    "    anom_ev = anom_calc_ubp(df, intracmnty_fname, usr, ws=10, sig=3)\n",
    "    if len(anom_ev['anomalies_dict']):\n",
    "        ubp_events_dic[key]=anom_ev\n",
    "\n",
    "    \n",
    "    # count += 1\n",
    "    # if count%20 == 0:\n",
    "    #     break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "f3_anom_user_lst=[]\n",
    "# for anom_usr in ubp_events_dic:\n",
    "#     a_ev = ubp_events_dic[anom_usr]\n",
    "#     print('dict a_ev=', a_ev['anomalies_dict'])\n",
    "#     for k, v in a_ev['anomalies_dict'].items():\n",
    "#         if v != 0:\n",
    "#             f1_anom_user_lst.append(anom_usr)\n",
    "#             break\n",
    "\n",
    "f3_anom_user_lst += get_anom_list(ubp_events_dic)\n",
    "f3_anom_user_lst += get_anom_list(pbp_events_dic)\n",
    "f3_anom_user_lst += get_anom_list(cbp_events_dic)\n",
    "\n",
    "len(set(f3_anom_user_lst))\n",
    "\n",
    "# save list of anom users\n",
    "pickle_store(\"pickle/f3/f3_anom_set_file\", set(f3_anom_user_lst)) #raw means not indexed by Timeindex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "649"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "len(set(cbp_events_dic))\n"
   ]
  }
 ]
}